{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aba04d4-eb1a-4f3c-99c9-3a7b6a9c1d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scraping CBE ===\n",
      "Collected 100/400 reviews for CBE\n",
      "Collected 200/400 reviews for CBE\n",
      "Collected 300/400 reviews for CBE\n",
      "Collected 400/400 reviews for CBE\n",
      "âœ… Success: Collected 400 reviews\n",
      "\n",
      "=== Scraping BOA ===\n",
      "Collected 100/400 reviews for BOA\n",
      "Collected 200/400 reviews for BOA\n",
      "Collected 300/400 reviews for BOA\n",
      "Collected 400/400 reviews for BOA\n",
      "âœ… Success: Collected 400 reviews\n",
      "\n",
      "=== Scraping Dashen ===\n",
      "Collected 100/400 reviews for Dashen\n",
      "Collected 200/400 reviews for Dashen\n",
      "Collected 300/400 reviews for Dashen\n",
      "Collected 400/400 reviews for Dashen\n",
      "âœ… Success: Collected 400 reviews\n",
      "\n",
      "=== Collection Summary ===\n",
      "bank\n",
      "CBE       400\n",
      "BOA       400\n",
      "Dashen    400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>bank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>very nice</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>CBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>safe easy &amp; fast</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>Dashen yichalal. Ewnetem one step a head</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>Dashen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>goid</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>CBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>always CBE is the leading Commercial BankðŸ’ªðŸ’ªðŸ’ª</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-18</td>\n",
       "      <td>CBE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review  rating        date    bank\n",
       "397                                      very nice       5  2025-03-31     CBE\n",
       "360                               safe easy & fast       5  2025-04-01     CBE\n",
       "1196      Dashen yichalal. Ewnetem one step a head       5  2025-01-17  Dashen\n",
       "211                                           goid       5  2025-04-28     CBE\n",
       "243   always CBE is the leading Commercial BankðŸ’ªðŸ’ªðŸ’ª       5  2025-04-18     CBE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from google_play_scraper import reviews, Sort\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Updated bank apps configuration\n",
    "apps = {\n",
    "    'CBE': 'com.combanketh.mobilebanking',\n",
    "    'BOA': 'com.boa.boaMobileBanking', \n",
    "    'Dashen': 'com.dashen.dashensuperapp'\n",
    "}\n",
    "\n",
    "def scrape_app_reviews(app_id, bank_name, target_count=400):\n",
    "    \"\"\"Scrape reviews with pagination to reach target count\"\"\"\n",
    "    all_reviews = []\n",
    "    continuation_token = None\n",
    "    batch_size = 100  # Max per request\n",
    "    retry_count = 0\n",
    "    \n",
    "    while len(all_reviews) < target_count and retry_count < 3:\n",
    "        try:\n",
    "            # Get reviews in batches\n",
    "            result, continuation_token = reviews(\n",
    "                app_id,\n",
    "                lang='en',\n",
    "                country='et',\n",
    "                sort=Sort.NEWEST,\n",
    "                count=batch_size,\n",
    "                continuation_token=continuation_token\n",
    "            )\n",
    "            \n",
    "            all_reviews.extend(result)\n",
    "            print(f\"Collected {len(all_reviews)}/{target_count} reviews for {bank_name}\")\n",
    "            \n",
    "            if not continuation_token:\n",
    "                break\n",
    "                \n",
    "            # Be polite with delay between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # Process the collected reviews\n",
    "    df = pd.DataFrame(all_reviews[:target_count])  # Trim to exact target\n",
    "    df['bank'] = bank_name\n",
    "    df['date'] = pd.to_datetime(df['at']).dt.date\n",
    "    \n",
    "    return df[['content', 'score', 'date', 'bank']].rename(columns={\n",
    "        'content': 'review',\n",
    "        'score': 'rating'\n",
    "    })\n",
    "\n",
    "# Scrape reviews for all banks (400 each)\n",
    "all_reviews = pd.DataFrame()\n",
    "\n",
    "for bank_name, app_id in apps.items():\n",
    "    print(f\"\\n=== Scraping {bank_name} ===\")\n",
    "    try:\n",
    "        bank_reviews = scrape_app_reviews(app_id, bank_name, 400)\n",
    "        all_reviews = pd.concat([all_reviews, bank_reviews], ignore_index=True)\n",
    "        print(f\"âœ… Success: Collected {len(bank_reviews)} reviews\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to scrape {bank_name}: {str(e)}\")\n",
    "\n",
    "# Save and verify results\n",
    "if not all_reviews.empty:\n",
    "    review_counts = all_reviews['bank'].value_counts()\n",
    "    print(\"\\n=== Collection Summary ===\")\n",
    "    print(review_counts)\n",
    "    \n",
    "    all_reviews.to_csv('bank_reviews.csv', index=False)\n",
    "    print(\"\\nSample reviews:\")\n",
    "    display(all_reviews.sample(5))\n",
    "else:\n",
    "    print(\"\\nNo reviews collected - please check configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5090d3-09b4-4d2a-ae98-c6c282b46782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1200 raw reviews\n",
      "Successfully saved 987 processed reviews to cleaned_reviews.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>bank</th>\n",
       "      <th>source</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20 years</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A great app. It's like carrying a bank in your...</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>great app like carrying bank pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>More than garrantty bank EBC.</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>garrantty bank ebc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>really am happy to this app it is Siple to use...</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>really happy app siple use everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I liked this app. But the User interface is ve...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>CBE</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>liked app user interface basic attractive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date bank  \\\n",
       "0                                           20 years       5  2025-06-08  CBE   \n",
       "1  A great app. It's like carrying a bank in your...       4  2025-06-07  CBE   \n",
       "2                      More than garrantty bank EBC.       4  2025-06-07  CBE   \n",
       "3  really am happy to this app it is Siple to use...       5  2025-06-07  CBE   \n",
       "4  I liked this app. But the User interface is ve...       2  2025-06-07  CBE   \n",
       "\n",
       "        source                               clean_review  \n",
       "0  Google Play                                       year  \n",
       "1  Google Play        great app like carrying bank pocket  \n",
       "2  Google Play                         garrantty bank ebc  \n",
       "3  Google Play      really happy app siple use everything  \n",
       "4  Google Play  liked app user interface basic attractive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self._setup_nltk()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = self._get_tokenizer()\n",
    "        \n",
    "    def _setup_nltk(self):\n",
    "        \"\"\"Ensure all required NLTK data is available\"\"\"\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "    \n",
    "    def _get_tokenizer(self):\n",
    "        \"\"\"Get the best available tokenizer with fallback\"\"\"\n",
    "        try:\n",
    "            # Test if advanced tokenization works\n",
    "            word_tokenize(\"test\")\n",
    "            return word_tokenize\n",
    "        except:\n",
    "            # Fallback to simple whitespace tokenizer\n",
    "            return WhitespaceTokenizer().tokenize\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Robust text cleaning with fallback tokenization\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        \n",
    "        # Tokenize with fallback\n",
    "        tokens = self.tokenizer(text)\n",
    "        \n",
    "        # Lemmatize and remove stopwords\n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens \n",
    "                 if t not in self.stop_words and len(t) > 2]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "def preprocess_reviews(input_path, output_path):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = TextPreprocessor()\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(input_path)\n",
    "        print(f\"Loaded {len(df)} raw reviews\")\n",
    "        \n",
    "        # Clean data\n",
    "        df = df.dropna(subset=['review']).drop_duplicates('review')\n",
    "        df['clean_review'] = df['review'].apply(preprocessor.clean_text)\n",
    "        df['source'] = 'Google Play'\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = ['review', 'rating', 'date', 'bank', 'source', 'clean_review']\n",
    "        df[final_columns].to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Successfully saved {len(df)} processed reviews to {output_path}\")\n",
    "        return df[final_columns].head()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "preprocess_reviews('bank_reviews.csv', 'cleaned_reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
